---
title: "autodiffr_iter_latest"
author: "seong hoon yoon"
date: '2022-10-14'
output: html_document
---

```{r}
library(pacman)
pacman::p_load(tidyverse,magrittr,flexsurv,dplyr,tidyr,tidyverse,data.table,plyr,TMB,survey,lme4)
```

```{r}
set.seed(3105)
# Number of subjects
N <- 5000 #increase this to 10000
sample_size <- 400
# Unconditional true parameters (Generalised)
alpha.true<- c(4,1,1,1) 
beta.true  <- c(2,1,1,1)
gamma.true <- c(1.5, 1, 1)
lambda1.true <- 0.7 
lambda2.true <- 0.5
lambda3.true <- 0.5
delta.true <- 0.5
sigmaA.true <- 1
sigmaB.true <- 1
# Replicates 
mean.repl <- 20 #changed from 4   (cluster size)
replicates <- rpois(N,mean.repl)+3
idK <- sort(rep(1:N, replicates))

Ntotal <-   sum(replicates)

X1 <- rbinom(Ntotal, 1, 0.3)
X2 <- rbinom(Ntotal, 1, 0.4) 
X3 <- rbinom(Ntotal, 1, 0.5)
X <- cbind(1,X1,X2,X3)
nparam_a <- ncol(X) + 1 # 4 alphas + logsigma_a
nparam_b <- ncol(X) + 3 # 4 betas + logsigma_a + lambda1 + delta

#for both binary and normal case?
Z1 <- rbinom(N, 1, 0.4) #no repeated measure for Z.  Z1 is directly usable in cpp 
Z2 <- rbinom(N, 1, 0.5) 
Z3 <- rbinom(N, 1, 0.6)
Z.1 <- Z1[idK]
Z.2 <- Z2[idK]
Z.3 <- Z3[idK]
Z  <- cbind(Z.1, Z.2, Z.3)
Z_one <- cbind(Z1, Z2, Z3)
nparam_c <- ncol(Z) + 12 # 3 gammas + lambda2 + lambda3 + 10 h's

#partially missing variable available only for phase 2 sample
#nth<-1
#nth_missing <- cumsum(c(nth,replicates))[1:N]

aux1<-rbinom(Ntotal,1,ifelse(X1== 1 | X2 ==1, 0.9,0.1)) # binomial case
aux2<-rbinom(Ntotal,1,ifelse(X1+X2>=1, 0.95,0.05)) # binomial case
aux3<- rbinom(Ntotal, 5, ifelse(X1==1, 0.9,0.1)) #discrete case
#aux4 <- rnorm(Ntotal, 5, 2)  # continuous case if required
aux <- cbind(aux1,aux2,aux3)
Z_aux1 <- rbinom(N, 1, ifelse(Z1==1, 0.9,0.1))
Z_aux2 <- rbinom(N, 1, ifelse(Z1==1, 0.9,0.1))
Z_aux3 <- rbinom(N, 1, ifelse(Z1==1, 0.95,0.05))
Z_aux <- cbind(Z_aux1, Z_aux2, Z_aux3) #matrix of aux vars for Z
#number of aux vars
numbaux <- 3


# Random effects
a<-rnorm(N,0,sigmaA.true)
b<-rnorm(N,0,sigmaB.true)
a_All <- a[idK]
b_All <- b[idK]


# Conditional true parameters   
eta.true <- X %*% alpha.true +a_All
mu.true <- exp(X %*% beta.true + lambda1.true*a_All + b_All)
sigmaiij2.true <- exp(delta.true)

# Prepare a data frame with identifiers and covariates
df <- data.frame(Identifier=idK,X=X[,-c(1)]) 
df$YNoZero  <-  rbinom(Ntotal, 1, 1/(1+exp(-eta.true)))

df$cost <- rgamma(Ntotal,shape=sigmaiij2.true, scale=mu.true/sigmaiij2.true)
df <- df %>% mutate(cost2 =  cost*YNoZero )
#df <- df %>% mutate(cost2 =  cost) 

#survival part

l0 <- 2  # hazard is exponential 
U2     <-   runif(N, 0,1)[idK]
idKu  <- unique(idK)
survt0 <-survt <- (-log(U2)/(l0*exp(Z %*% gamma.true + lambda2.true*a_All + lambda3.true*b_All)))

# censoring times:
survt.censor <- rexp(N, rate= 2)[idK]
# censoring:
di    <- as.numeric(survt0 <= survt.censor);
survt <- pmin(survt0, survt.censor)  #survt is the minimum of survival time and censoring time
df$survt <- survt
df$di <- di

#df$cost  <- df$cost* df$di

df$idK <- idK
df$num <- unlist(sapply(1:N, function(v) 1:replicates[v]))

#auxiliary
df <- cbind(df,aux)

dfS <- df[!duplicated(df$idK, fromLast = TRUE),]

survt1<-dfS$survt #one survt per indiv
di1 <- dfS$di
replicates <- dfS$num #replicates after removing obs after event


maxrep<-max(replicates)
numbX<-ncol(X)-1 #excluding the first column of 1's


setDT(df)

#dcast(df, Identifier~num, value.var = c("cost2","X1","X2","X3"))
df1 <- cbind(dcast(df, Identifier~num, value.var = c("cost2",paste0("X.X",1:numbX),paste0("aux",1:numbaux))),dfS[,c("survt","di","num","YNoZero")]) #complete wide-form dataset with nrow=N


start<-cumsum(c(1,replicates))
end<-cumsum(replicates)
#modular now
df1$X <- rowMeans(sapply(1:numbX, function(i) sapply(1:N, function(v)
   mean(get(paste0("X",i))[start[v]:end[v]])
)))


ncoldf <-ncol(df1)
nX1col <- (1:ncoldf)[colnames(df1) %in% paste('cost2', 1:maxrep, sep='_')]
dfcost<-as.matrix(df1[,nX1col , with = FALSE])
costindsum<-list()
costsum<-list()
for(i in 1:N){
  costindsum[[i]] <- sum(dfcost[i,][1:replicates[i]] > 0)
  costsum[[i]] <- sum(dfcost[i,][1:replicates[i]] )
}
df1$costindexsum <- unlist(costindsum)
df1$costsum <- unlist(costsum)
df1 <- cbind(df1,Z_one,Z_aux)
```


```{r}
#Sampling from population
n.strata <- 10 #each stratum has 10 clusters
n.cluster <- 10 #for each cluster within a stratum there are 200 obs.  A total of 50 clusters.
n.per.cluster <- 20 # n.per.cluster * 2 * n.strata = sample size   #this adjusts the weight
total.cluster <- n.strata * n.cluster
df1$strata<-rep(seq(1:n.strata), each=N/n.strata) 
df1$cluster<-rep(seq(1:n.cluster),N/10)

samplingfunc <- function(iter,data){
  #phase1.df1=list() # phase1 sample   where two clusters are selected from each stratum
  #phase1.df2=list()
  phase2.df1 = list()
  phase2.df2 = list()
  datalist1=list()
  datalist2=list() 
  X.rowlist<-list()
  costlist<-list()
  datares<-list()
  for(i in 1:n.strata){
    strata.df <- subset(data, data$strata == i) #each stratum has 500 
    cluster.label <- sample(c(1:n.cluster), 2, prob=rep(1/n.cluster,n.cluster))
    
    cluster.df1 <- subset(strata.df, strata.df$cluster == cluster.label[1])
    cluster.df2 <- subset(strata.df, strata.df$cluster == cluster.label[2])
    
    cluster.df1$size <- (0.25+0.5*cluster.df1$X)*0.5  #size measure
    cluster.df1$pps <- cluster.df1$size / sum(cluster.df1$size) #selection prob
    #cluster.df1$wght <- 1/cluster.df1$prob    #sampling wght
    
    
    cluster.df2$size<- (0.25+0.5*cluster.df2$X)*0.5 #size measure
    cluster.df2$pps <- cluster.df2$size / sum(cluster.df2$size) #pps
    #cluster.df2$wght <- 1/cluster.df2$prob   #sampling wght
    
    indiv.selected <- sample(c(1:(N/total.cluster)),size=n.per.cluster,prob=c(cluster.df1$prob),
    replace=FALSE)
    #select 4 individuals based on selection probs
    cluster.df1.indiv <- cluster.df1[indiv.selected,]
    cluster.df1.indiv$prob <- 0.2 * (n.per.cluster*cluster.df1.indiv$size)/ sum(cluster.df1$size)
    cluster.df1.indiv$weight <- 1/cluster.df1.indiv$prob
    
    indiv.selected2 <- sample(c(1:(N/total.cluster)),size=n.per.cluster,prob=c(cluster.df2$prob),replace=FALSE)
    cluster.df2.indiv <- cluster.df2[indiv.selected2,]
    cluster.df2.indiv$prob <- 0.2 * (n.per.cluster*cluster.df2.indiv$size)/ sum(cluster.df2$size)
    cluster.df2.indiv$weight <- 1/cluster.df2.indiv$prob
    
    # Xrow subset using identifier cbind(data[ident,],Xlist[[1]][ident,])
    
    datalist1[[i]] <- cluster.df1.indiv
    datalist2[[i]] <- cluster.df2.indiv
    #sample further from cluster.df1 and df.2
    phase2.selected1 <- sample(c(1:n.per.cluster), size=n.per.cluster/2, prob=rep(1/n.per.cluster, n.per.cluster),replace=FALSE)
    phase2.selected2 <- sample(c(1:n.per.cluster), size=n.per.cluster/2, prob=rep(1/n.per.cluster, n.per.cluster),replace=FALSE)
    
    phase2.data1 <- cluster.df1.indiv[phase2.selected1,]
    phase2.data2 <- cluster.df2.indiv[phase2.selected2,]
    phase2.data1$weight2 <- phase2.data1$weight * 2
    phase2.data2$weight2 <- phase2.data2$weight * 2
    phase2.df1[[i]] <- phase2.data1
    phase2.df2[[i]] <- phase2.data2
    

  }
  
  data1 <- do.call("rbind", datalist1)
  data2 <- do.call("rbind", datalist2)

  phase2_1 <- do.call("rbind", phase2.df1)
  phase2_2 <- do.call("rbind", phase2.df2) 
  phase2 <- rbind(phase2_1, phase2_2)
  data.final <-  rbind(data1,data2)
  
  datares<-list(data.final)
  phase2<-list(phase2)

  list(datares=datares,phase2=phase2)
}
# sampling  n times to generate dataset
par_est <- list()
n.iter<-1
sampledata<-lapply(1:n.iter, function(v) samplingfunc(v,df1))

```


```{r}
#individual laplace cpp templates

compile("la_general.cpp",framework="TMBad")
dyn.load(dynlib("la_general"))

compile("lb_general.cpp",framework="TMBad")
dyn.load(dynlib("lb_general"))

#no randeff
compile("lc_general.cpp")
dyn.load(dynlib("lc_general"))

#compile("lb_nolambda.cpp",framework="TMBad") #dyn.load(dynlib("lb_nolambda"))
```


#Variance estimation function for weighted 
```{r}
var_est <- function(data,gr_all, nparam_a,nparam_b,nparam_c,weights,sample_size,n.indiv,num.cluster){
  param_list<-list()
  nparam <- nparam_a + nparam_b + nparam_c
  for(i in 1:nparam){
    #a_nam <- c("logsigma_a_gr", paste0("alpha",0:(nparam_a-1),"_gr"))
    #assign(paste0("theta",i), repsum[(sample_size*i+1):(sample_size*(i+1))])
    #assign(a_nam[i], repsum[(sample_size*i+1):(sample_size*(i+1))])
    param_list[[i]] <- gr_all[(sample_size*i+1):(samples_size*(i+1))]
  }
  mat_gr <- data.frame(do.call(cbind,param_list),data[[1]]$datares[[1]]$cluster,data[[1]]$datares[[1]]$strata, data[[1]]$datares[[1]]$weight)
  mat_gr1 <- weights*mat_gr[1:(sample_size/2),]; mat_gr2 <- weights*mat_gr[(sample_size/2+1):sample_size,]
  #cpp template is returning unweighted  gradient
  
  #variance estimation
var_ind <- seq(1,sample_size+1,n.indiv)
G <- matrix(0,nrow=nparam, ncol=nparam)
for(i in 1:n.strata){ #n.strata = 100
  e_hl_1 <- colSums(mat_gr1[,1:nparam][var_ind[i]:var_ind[i+1]-1,]) #1st cluster sum in str h
  e_hl_2 <- colSums(mat_gr2[,1:nparam][var_ind[i]:var_ind[i+1]-1,]) #2nd cluster sum in str h

  e_h <- (1/num.cluster) * (e_hl_1 + e_hl_2) #colsum returns a vector
  G <- G + (num.cluster / (num.cluster -1)) * ((as.matrix(e_hl_1 - e_h) %*% t(as.matrix(e_hl_1 - e_h)))
  + (as.matrix(e_hl_2 - e_h) %*% t(as.matrix(e_hl_2 - e_h))))
  
}
invhess <- solve(opt$hessian) #this is weighted Hessian
se <- sqrt(diag(invhess %*% G %*% invhess)) 
return(se)
}
```

```{r}
is.na(opt$value)
sampledata[[1]]$datares[[1]]
```


# the entire estimation using a single function
```{r}
#unweighted
adfun(dat=sampledata,n.iter=1000,sample_size,nparam_a,nparam_b,nparam_c,inf_fun, n.indiv, num.cluster, type="unweighted")
#weighted
adfun(dat=sampledata,n.iter=1000,sample_size,nparam_a,nparam_b,nparam_c,inf_fun, n.indiv, num.cluster, type="weighted")
#calibrated
adfun(dat=sampledata,n.iter=1000,sample_size,nparam_a,nparam_b,nparam_c,inf_fun, n.indiv, num.cluster, type="calibrated")
```

```{r}
data.frame(tst$par)
```

```{r}
tst <- adfun(dat=sampledata,n.iter=1,sample_size,nparam_a,nparam_b,nparam_c,inf_fun, n.indiv, num.cluster, type="weighted")
```

```{r}
#add...
# 1) opt$par mat & averaging
# 2) var return
adfun <- function(dat,n.iter,sample_size, nparam_a,nparam_b,nparam_c, inf_fun,n.indiv, num.cluster, type) { 
  
  # data passed to this function is a list of data frames (1000)
  # can have sampling function here  so that  sample size can be also modular (by + or - the number of subjects selected in each cluster) if needed later
  # make the function extract gradient functions only when the type is weighted or calibrated because gradient functions aren't required for unweighted analysis
parest_a <- matrix(NA,nrow=n.iter,ncol=nparam_a)
parest_b <- matrix(NA,nrow=n.iter,ncol=nparam_b)
parest_c <- matrix(NA,nrow=n.iter,ncol=nparam_c)
na_ind_a <- c()
na_ind_b <- c()
na_ind_c <- c()
SE <- matrix(NA,nrow=n.iter,ncol=nparam_a+nparam_b+nparam_c) #separately or jointly?
SE_a <- matrix(NA,nrow=n.iter,ncol=nparam_a)
SE_b <- matrix(NA,nrow=n.iter,ncol=nparam_b)
SE_c <- matrix(NA,nrow=n.iter,ncol=nparam_c)

for(i in 1:n.iter){

  
xmat<-list()
for(j in 1:numbX){
  assign(paste0("colname_X", j), (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste0('X.X', j, "_",1:maxrep)])
  assign(paste0("X",j,"row"),as.matrix(dat[[i]]$datares[[1]][,get(paste0("colname_X",j)) , with = FALSE]) )
  xmat[[j]] <- as.vector(t(get(paste0("X",j,"row"))))
}

xmat<-do.call("cbind",xmat)
X_sample<-cbind(1,xmat[!rowSums(!is.finite(xmat)),])

ncoldfZ <-ncol(dat[[i]]$datares[[1]])
colname_costZ <- (1:ncoldfZ)[colnames(dat[[i]]$datares[[1]]) %in% paste0('Z', 1:numbX)]
zmat <- as.matrix(dat[[i]]$datares[[1]][,colname_costZ, with =FALSE])

# cost

ncoldf <-ncol(dat[[i]]$datares[[1]])
colname_cost <- (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste('cost2', 1:maxrep, sep='_')]

dfcost<-as.matrix(dat[[i]]$datares[[1]][,colname_cost , with = FALSE])
survt1 <-   dat[[i]]$datares[[1]]$survt
di1 <-  dat[[i]]$datares[[1]]$di
costindexsum<- dat[[i]]$datares[[1]]$costindexsum
costsum<- dat[[i]]$datares[[1]]$costsum
if(type=="weighted"){
weight<- dat[[i]]$datares[[1]]$weight
}
if(type=="unweighted"){
  weight<-rep(1,sample_size)
}
qq <-10  #10 intervals
Q.partition <- c(0,quantile(survt1,1:qq/qq)) # length 11, need to be calculated for entire sample each iteration
cumhaz_int <- c(Q.partition[2],(Q.partition[3:(qq+1)]-Q.partition[(2):(qq)]))
index_vec<-sapply(1:sample_size, function(v) min((1:length(Q.partition)-2)[survt1[v]<=Q.partition]))
#(1:length(Q.partition))[survt1[1]<=Q.partition]) 

replicates <- dat[[i]]$datares[[1]]$num  # num. repeated measurements per subject
maxrep_tmb <- max(replicates)
start<-c(0,cumsum(replicates))

if (type =="calibrated"){
  cal <- calibrate_fn(...) #make this modular
  weights <- weights(cal) #these weights will be used for optimization  regardless of the pre-declared values in the weight
}

data <- list(Xrow=X_sample, N=sample_size, replicates=replicates, dfcost=dfcost, weights=weight,a1=0, start=start)
parameters <- list(sigma=c(0,1,1,1,1))  # sigma=c(0,rep(1,nparam_a-1))

obj <- MakeADFun(data,parameters,DLL="la_general", method="L-BFGS-B") 
#L-BFGS-B always works for lA when it doesnt work without it, LB works fine for weights
obj$hessian <- TRUE
opt<-suppressWarnings(do.call("optim",obj))
parest_a[i,] <- opt$par #parameter estimates
na_ind_a[i] <- is.na(opt$value)

if(is.na(opt$value)){
  next
}

rep<-sdreport(obj)
repsum <- summary(rep, "report") #estimated a's
a_est<-repsum[1:sample_size]
la_gr <- repsum[(sample_size+1):(sample_size*nparam_a),] # excluding a's, only the gr fn
if (type == "unweighted"){
  SE_a[i,] <- sqrt(diag(solve(opt$hessian)))  # check whether logsigma need to be exponentiated 
}

#model B individual laplace
data <- list(Xrow=X_sample, N=sample_size, replicates=replicates, dfcost=dfcost, weights=weight,a1=a_est, b1=0, costindexsum=costindexsum, costsum=costsum, start=start)

parameters <- list(sigma=c(0,1,1,1,1,1,1)) #make this modular  use eg nparam_b and rep

obj <- MakeADFun(data,parameters,DLL="lb_general")
obj$hessian <- TRUE
opt<-suppressWarnings(do.call("optim",obj))
parest_b[i,] <- opt$par
na_ind_b[i] <- is.na(opt$value)

if(is.na(opt$value)){
  next
}

rep<-sdreport(obj)
repsum <- summary(rep, "report") #estimated b's
b_est<-repsum[1:sample_size]
lb_gr <- repsum[(sample_size+1):(sample_size*nparam_b),]
#need some kind of indicator to  know whether the estimation is successful  (Eg value=?) in case grad is NaN
if (type == "unweighted"){
  SE_b[i,] <- sqrt(diag(solve(opt$hessian)))  # check whether logsigma need to be exponentiated 
}
#lC

data<-list(a1=a_est,b1=b_est, Z0=zmat,N=sample_size,Qpartition=Q.partition,survt1=survt1,di1=di1,cumhaz_int=cumhaz_int,index_vec=index_vec,weights=weight)

parameters <- list(theta=c(rep(1,nparam_c))) #list(gamma=rep(1,ncol(Z)),lambda2=1,lambda3=1,h1=1,h2=1,h3=1,h4=1,h5=1,h6=1,h7=1,h8=1,h9=1,h10=1)

obj <- MakeADFun(data,parameters,DLL="lc_general")
obj$hessian <- TRUE
opt <- suppressWarnings(do.call("optim",obj))
parest_c[i,] <- opt$par
na_ind_c[i] <- is.na(opt$value)
rep<-sdreport(obj)
repsum <- summary(rep, "report")
lc_gr <- repsum[(sample_size+1):(sample_size*nparam_c),]

gr_all <- rbind(la_gr,lb_gr,lc_gr)

if (type == "unweighted"){
  SE_c[i,] <- sqrt(diag(solve(opt$hessian)))  # check whether logsigma need to be exponentiated 
}
if (type == "weighted"){
  #variance estimation using the G matrix and inverted Hessian
  SE[i,] <- var_est(data,gr_all, nparam_a,nparam_b,nparam_c,weights,sample_size,n.indiv,num.cluster)
}




} #end of loop
  #return output..
  par <- data.frame(cbind(parest_a,parest_b,parest_c,na_ind_a,na_ind_b,na_ind_c))
  par_SE <- list(par=par, SE=SE)
  return(par_SE)
}
```

```{r}
dim(iff)
lm(repsum[4801:5200,]~iff[,1])
```

#quick test for cox calib
```{r}
a_est<-rnorm(N,0,1); b_est<-rnorm(N,0,1)
Z_nam <- c(paste0("Z",1:numbX),"offset(a_est)", "offset(b_est)")
fmla <- as.formula(paste("Surv(survt1,di1)~",paste(Z_nam,collapse="+")))  #use a's and b's as offsets? 
lc_imp <- coxph(fmla)
iff<-resid(lc_imp,type="dfbeta")
iff

ncoldfZ <-ncol(dat[[i]]$datares[[1]])
colname_costZ <- (1:ncoldfZ)[colnames(dat[[i]]$datares[[1]]) %in% paste0('Z', 1:numbX)]
zmat <- as.matrix(dat[[i]]$datares[[1]][,colname_costZ, with =FALSE])
survt1 <-   dat[[i]]$datares[[1]]$survt
di1 <-  dat[[i]]$datares[[1]]$di
costindexsum<- dat[[i]]$datares[[1]]$costindexsum
costsum<- dat[[i]]$datares[[1]]$costsum
weight<- dat[[i]]$datares[[1]]$weight

qq <-10  #10 intervals
Q.partition <- c(0,quantile(survt1,1:qq/qq)) # length 11, need to be calculated for entire sample each iteration
cumhaz_int <- c(Q.partition[2],(Q.partition[3:(qq+1)]-Q.partition[(2):(qq)]))
index_vec<-sapply(1:sample_size, function(v) min((1:length(Q.partition)-2)[survt1[v]<=Q.partition]))

i=1
lc_des <- svydesign(id=~1, weights=~weight2, strata=NULL, data=dat[[i]]$phase2[[1]])
z_nam <- paste0("Z",1) # Z1
z_auxnam <- paste0("Z_aux",1:numbaux)
fmla <- as.formula(paste(get("z_nam"),"~",paste(z_auxnam,collapse="+")))
lc_glm <- svyglm(fmla, family=binomial, design=lc_des) #predicton model using phase2 data

lc_fit <- summary(lc_glm) #use this to impute pmv
ncoldf <-ncol(dat[[i]]$datares[[1]])
colname_Z_aux <- (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste0('Z_aux', 1:numbaux)]
Z_auxmat <- as.matrix(dat[[i]]$datares[[1]][,colname_Z_aux , with = FALSE])
Z_auxmat <- cbind(1,Z_auxmat)
xb <- Z_auxmat  %*% lc_fit$coefficients[,1]
dat[[i]]$datares[[1]]$imp_Z1 <- ifelse(exp(xb)/(1+exp(xb))>0.5,1,0) 

#a_est<-rnorm(sample_size,0,1); b_est <- rnorm(sample_size,0,1)

Z_nam <- c("imp_Z1",paste0("Z",2:numbX),"offset(a_est)", "offset(b_est)")
fmla <- as.formula(paste("Surv(survt,di)~",paste(Z_nam,collapse="+")))  #use a's and b's as offsets? 
lc_imp <- coxph(fmla, data=dat[[i]]$datares[[1]])
#lc_imp <- coxph()
inf_fun <- resid(lc_imp,  type='dfbeta')  #this is the IF for cox model
colnames(inf_fun) <- paste0("if",1:ncol(inf_fun))
des0 <- svydesign(id=~0, weights=~weight, data=dat[[i]]$datares[[1]])
calformula <- make.formula(colnames(inf_fun))
des0if <- update(des0, if1 = inf_fun[,1],
if2=inf_fun[,2], if3=inf_fun[,3])

cal <- calibrate(des0if, formula=~if1+if2+if3, pop=c(N,0,0,0)) #put other ifs?
cal_w_c <- weights(cal)
cal_w_c[1:10]

length(a_est); length(b_est);
dim(zmat); sample_size;
length(Q.partition); length(survt1);  length(di1);
length(cumhaz_int);
length(index_vec);
length(cal_w_c)

data<-list(a1=a_est,b1=b_est, Z0=zmat,N=sample_size,Qpartition=Q.partition,survt1=survt1,di1=di1,cumhaz_int=cumhaz_int,index_vec=index_vec,
           weights=sampledata[[1]]$datares[[1]]$weight) #put c-weights

parameters <- list(theta=c(rep(1,nparam_c))) #list(gamma=rep(1,ncol(Z)),lambda2=1,lambda3=1,h1=1,h2=1,h3=1,h4=1,h5=1,h6=1,h7=1,h8=1,h9=1,h10=1)

obj <- MakeADFun(data,parameters,DLL="lc_general")
obj$hessian <- TRUE
opt <- suppressWarnings(do.call("optim",obj))
#opt$par
rep<-sdreport(obj)
repsum <- summary(rep, "report")
lc_gr <- repsum[(sample_size*12+1):(sample_size*nparam_c)]

gr_gam1 <- lc_gr[1:sample_size];  #this is already c-weighted
gr_gam2 <- lc_gr[(sample_size+1):(2*sample_size)]
gr_gam3 <- lc_gr[(2*sample_size+1):(3*sample_size)]
mat_gr <- cbind(gr_gam1,gr_gam2,gr_gam3)
mat_gr1 <- mat_gr[1:(sample_size/2),];
mat_gr2 <- mat_gr[(sample_size/2+1):sample_size,]

fit_gam1 <- lm(gr_gam1~inf_fun[,1]) #IF also need to be weighted??
fit_gam2 <- lm(gr_gam2~inf_fun[,2])
fit_gam3 <- lm(gr_gam3~inf_fun[,3])

#score_gam1 <- resid(fit_gam1, type="response")  #or fit_gam1$fitted.values ?
#score_gam2 <- resid(fit_gam2, type="response")
#score_gam3 <- resid(fit_gam3, type="response")    

score_gam1 <- fit_gam1$fitted.values
score_gam2 <- fit_gam2$fitted.values
score_gam3 <- fit_gam3$fitted.values
score_est <- cbind(score_gam1,score_gam2,score_gam3)
score_est1 <- score_est[1:(sample_size/2),]
score_est2 <- score_est[(sample_size/2+1):sample_size,]

#inf_fun_tst <- cbind(score_alp0, score_alp1, score_alp2, score_alp3)
#inf_fun1 <- cbind(inf_fun_tst[ind,1],inf_fun_tst[ind,2],inf_fun_tst[ind,3],inf_fun_tst[ind,4])
ngamma=3

#variance estimation : gradients are unweigthed in c++  so need to multiply the weights
n.indiv <- 10
var_ind <- seq(1,sample_size/2+1,n.indiv)
length(var_ind)
num.cluster <- 2
G <- matrix(0,nrow=ngamma, ncol=ngamma)
for(i in 1:n.strata){ #n.strata = 20
  #e_hl_1 <- colSums(mat_gr1[var_ind[i]:var_ind[i+1]-1,] - score_est1[var_ind[i]:var_ind[i+1]-1,])  #1st cluster sum in str h
  #e_hl_2 <- colSums(mat_gr2[var_ind[i]:var_ind[i+1]-1,] - score_est2[var_ind[i]:var_ind[i+1]-1,]) #2nd cluster sum in str h
  e_hl_1 <- colSums(mat_gr1[var_ind[i]:var_ind[i+1]-1,])
  e_hl_2 <- colSums(mat_gr2[var_ind[i]:var_ind[i+1]-1,])
  e_h <- (1/num.cluster) * (e_hl_1 + e_hl_2) #colsum returns a vector
  G <- G + (num.cluster / (num.cluster -1)) * ((as.matrix(e_hl_1 - e_h) %*% t(as.matrix(e_hl_1 - e_h)))
  + (as.matrix(e_hl_2 - e_h) %*% t(as.matrix(e_hl_2 - e_h))))
  
}
#invhess <- solve(opt$hessian) #make sure this is the weighted hessian
invhess <- solve(opt$hessian[-c(1:12),-c(1:12)])
sqrt(diag(invhess %*% G %*% invhess))

```
weighted 0.11421069, 0.08415403, 0.09837279
calibrated-weighted 0.10423768, 0.08294581, 0.09093014

#calibration for suvival
```{r}
calibrate_surv <- function(dat,n.iter,sample_size, nparam_a,nparam_b,nparam_c, n.indiv, num.cluster) { 
  
for(i in 1:n.iter){
xmat<-list()
for(j in 1:numbX){
  assign(paste0("colname_X", j), (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste0('X.X', j, "_",1:maxrep)])
  assign(paste0("X",j,"row"),as.matrix(dat[[i]]$datares[[1]][,get(paste0("colname_X",j)) , with = FALSE]) )
  xmat[[j]] <- as.vector(t(get(paste0("X",j,"row"))))
}

xmat<-do.call("cbind",xmat)
X_sample<-cbind(1,xmat[!rowSums(!is.finite(xmat)),])

ncoldfZ <-ncol(dat[[i]]$datares[[1]])
colname_costZ <- (1:ncoldfZ)[colnames(dat[[i]]$datares[[1]]) %in% paste0('Z', 1:numbX)]
zmat <- as.matrix(dat[[i]]$datares[[1]][,colname_costZ, with =FALSE])

# cost

ncoldf <-ncol(dat[[i]]$datares[[1]])
colname_cost <- (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste('cost2', 1:maxrep, sep='_')]

dfcost<-as.matrix(dat[[i]]$datares[[1]][,colname_cost , with = FALSE])
survt1 <-   dat[[i]]$datares[[1]]$survt
di1 <-  dat[[i]]$datares[[1]]$di
costindexsum<- dat[[i]]$datares[[1]]$costindexsum
costsum<- dat[[i]]$datares[[1]]$costsum
weight<- dat[[i]]$datares[[1]]$weight

qq <-10  #10 intervals
Q.partition <- c(0,quantile(survt1,1:qq/qq)) # length 11, need to be calculated for entire sample each iteration
cumhaz_int <- c(Q.partition[2],(Q.partition[3:(qq+1)]-Q.partition[(2):(qq)]))
index_vec<-sapply(1:sample_size, function(v) min((1:length(Q.partition)-2)[survt1[v]<=Q.partition]))
#(1:length(Q.partition))[survt1[1]<=Q.partition]) 

replicates <- dat[[i]]$datares[[1]]$num  # num. repeated measurements per subject
maxrep_tmb <- max(replicates)
start<-c(0,cumsum(replicates))



data <- list(Xrow=X_sample, N=sample_size, replicates=replicates, dfcost=dfcost, weights=rep(1,sample_size),a1=0, start=start)
parameters <- list(sigma=c(0,1,1,1,1))  # sigma=c(0,rep(1,nparam_a-1))

obj <- MakeADFun(data,parameters,DLL="la_general")
obj$hessian <- TRUE
opt<-suppressWarnings(do.call("optim",obj))
#opt$par #parameter estimates
rep<-sdreport(obj)
repsum <- summary(rep, "report") #estimated a's
a_est<-repsum[1:sample_size]
#la_gr <- repsum[(sample_size+1):(sample_size*nparam_a)] # excluding a's, only the gr fn


#model B individual laplace
data <- list(Xrow=X_sample, N=sample_size, replicates=replicates, dfcost=dfcost, weights=rep(1,sample_size),a1=a_est, b1=0, costindexsum=costindexsum, costsum=costsum, start=start)

parameters <- list(sigma=c(0,1,1,1,1,1,1)) #make this modular  use eg nparam_b and rep

obj <- MakeADFun(data,parameters,DLL="lb_general")
obj$hessian <- TRUE
opt<-suppressWarnings(do.call("optim",obj))
#opt$par

rep<-sdreport(obj)
repsum <- summary(rep, "report") #estimated b's
b_est<-repsum[1:sample_size]
#lb_gr <- repsum[(sample_size+1):(sample_size*nparam_b)]
#need some kind of indicator to  know whether the estimation is successful  (Eg value=?) in case grad is NaN

#lC
#using the randeffs estimated above, fit a imputed cox model
lc_des <- svydesign(id=~1, weights=~weight2, strata=NULL, data=dat[[i]]$phase2[[1]])
z_nam <- paste0("Z",missing_z) # Z1
z_auxnam <- paste0("Z_aux",1:numbaux)
fmla <- as.formula(paste(get("z_nam"),"~",paste(z_auxnam,collapse="+")))
lc_glm <- svyglm(fmla, family=binomial, design=lc_des) #predicton model using phase2 data

lc_fit <- summary(lc_glm) #use this to impute pmv
ncoldf <-ncol(dat[[i]]$datares[[1]])
colname_Z_aux <- (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste0('Z_aux', 1:numbaux)]
Z_auxmat <- as.matrix(dat[[i]]$datares[[1]][,colname_Z_aux , with = FALSE])
Z_auxmat <- cbind(1,Z_auxmat)
xb <- Z_auxmat  %*% lc_fit$coefficients[,1]
dat[[i]]$datares[[1]]$imp_Z1 <- ifelse(exp(xb)/(1+exp(xb))>0.5,1,0) 
Z_nam <- c("imp_Z1",paste0("Z",2:numbX),"offset(a_est)", "offset(b_est)")
fmla <- as.formula(paste("Surv(survt,di)~",paste(Z_nam,collapse="+")))  #use a's and b's as offsets? 
lc_imp <- coxph(fmla, data=dat[[i]]$datares[[1]])
inf_fun <- resid(lc_imp,  type='dfbeta')  #this is the IF for cox model
colnames(inf_fun) <- paste0("if",1:ncol(inf_fun))
des0 <- svydesign(id=~0, weights=~weight, data=dat[[i]]$datares[[1]])
calformula <- make.formula(colnames(inf_fun))
des0if <- update(des0, if1 = inf_fun[,1],
if2=inf_fun[,2], if3=inf_fun[,3])

cal <- calibrate(des0if, formula=~if1+if2+if3, pop=c(N,0,0,0)) #put other ifs?
cal_w_c <- weights(cal)

#fit the 'actual' survival model 
data<-list(a1=a_est,b1=b_est, Z0=zmat,N=sample_size,Qpartition=Q.partition,survt1=survt1,di1=di1,cumhaz_int=cumhaz_int,index_vec=index_vec,
           weights=cal_w_c) #put c-weights

parameters <- list(theta=c(rep(1,nparam_c))) #list(gamma=rep(1,ncol(Z)),lambda2=1,lambda3=1,h1=1,h2=1,h3=1,h4=1,h5=1,h6=1,h7=1,h8=1,h9=1,h10=1)

obj <- MakeADFun(data,parameters,DLL="lc_general")
obj$hessian <- TRUE
opt <- suppressWarnings(do.call("optim",obj))
#opt$par
rep<-sdreport(obj)
repsum <- summary(rep, "report")
lc_gr <- repsum[(sample_size*12+1):(sample_size*nparam_c)] #use this to calculate calibrated variance
#exclude 10  hazs, cannot calibrate lambdas as randeffs are offsets

#calibrated var est- fix!
gr_gam1 <- lc_gr[1:sample_size];  #these are not weighed yet due to c++ code 
gr_gam2 <- lc_gr[(sample_size+1):(2*sample_size)]
gr_gam3 <- lc_gr[(2*sample_size+1):(3*sample_size)]
mat_gr <- cbind(gr_gam1,gr_gam2,gr_gam3)
mat_gr1 <- mat_gr[1:(sample_size/2),];
mat_gr2 <- mat_gr[(sample_size/2+1):sample_size,]

fit_gam1 <- lm(gr_gam1~inf_fun[,1]) #IF also need to be weighted??
fit_gam2 <- lm(gr_gam2~inf_fun[,2])
fit_gam3 <- lm(gr_gam3~inf_fun[,3])

score_gam1 <- resid(fit_gam1, type="response")  #or fit_gam1$fitted.values ?
score_gam2 <- resid(fit_gam2, type="response")
score_gam3 <- resid(fit_gam3, type="response")

score_est <- cbind(score_gam1,score_gam2,score_gam3)
score_est1 <- score_est[1:(sample_size/2),]
score_est2 <- score_est[(sample_size/2+1):sample_size,]

#inf_fun_tst <- cbind(score_alp0, score_alp1, score_alp2, score_alp3)
#inf_fun1 <- cbind(inf_fun_tst[ind,1],inf_fun_tst[ind,2],inf_fun_tst[ind,3],inf_fun_tst[ind,4])
ngamma=3

#variance estimation
n.indiv <- 10
var_ind <- seq(1,sample_size/2+1,n.indiv)
num.cluster <- 2
G <- matrix(0,nrow=ngamma, ncol=ngamma)
for(i in 1:n.strata){ #n.strata = 20
  e_hl_1 <- colSums(mat_gr1[var_ind[i]:var_ind[i+1]-1,] - score_est1[var_ind[i]:var_ind[i+1]-1,])  #1st cluster sum in str h
  e_hl_2 <- colSums(mat_gr2[var_ind[i]:var_ind[i+1]-1,] - score_est2[var_ind[i]:var_ind[i+1]-1,]) #2nd cluster sum in str h

  e_h <- (1/num.cluster) * (e_hl_1 + e_hl_2) #colsum returns a vector
  G <- G + (num.cluster / (num.cluster -1)) * ((as.matrix(e_hl_1 - e_h) %*% t(as.matrix(e_hl_1 - e_h)))
  + (as.matrix(e_hl_2 - e_h) %*% t(as.matrix(e_hl_2 - e_h))))
  
}
invhess <- solve(opt$hessian) #make sure this is the weighted hessian
invhess_gam <- invhess[-c(1:12),-c(1:12)]
sqrt(diag(invhess_gam %*% G %*% invhess_gam))

  
}
}
```



#dim check
dim(X_sample)
length(start)
length(replicates)
sum(replicates)
length(replicates)
dim(dfcost)
sample_size

#model A individual laplace
data <- list(Xrow=X_sample, N=sample_size, replicates=replicates, dfcost=dfcost, weights=rep(1,sample_size),a1=0, start=start)
parameters <- list(sigma=c(0,1,1,1,1))
obj <- MakeADFun(data,parameters,DLL="la_general")
obj$hessian <- TRUE
opt<-suppressWarnings(do.call("optim",obj))
opt$par
rep<-sdreport(obj)
repsum <- summary(rep, "report") #estimated a's
a_est<-repsum[1:sample_size]

#model B individual laplace
data <- list(Xrow=X_sample, N=sample_size, replicates=replicates, dfcost=dfcost, weights=rep(25,sample_size),a1=a_est, b1=0, costindexsum=costindexsum, costsum=costsum, start=start)
parameters <- list(sigma=c(0,1,1,1,1,1,1))
obj <- MakeADFun(data,parameters,DLL="lb_general")
obj$hessian <- TRUE
opt<-suppressWarnings(do.call("optim",obj))
opt$par
rep<-sdreport(obj)
repsum <- summary(rep, "report") #estimated b's
b_est<-repsum[1:sample_size]

#model C : no laplace
data<-list(a1=a_est,b1=b_est, Z0=zmat,N=sample_size,Qpartition=Q.partition,survt1=survt1,di1=di1,cumhaz_int=cumhaz_int,index_vec=index_vec,weights=rep(1,sample_size))
parameters <- list(theta=c(rep(1,15)))
#list(gamma=rep(1,ncol(Z)),lambda2=1,lambda3=1,h1=1,h2=1,h3=1,h4=1,h5=1,h6=1,h7=1,h8=1,h9=1,h10=1)
obj <- MakeADFun(data,parameters,DLL="lc_general")
obj$hessian <- TRUE
opt <- suppressWarnings(do.call("optim",obj))
opt$par
rep<-sdreport(obj)
repsum <- summary(rep, "report")

#manual calibration code 
la_des<-svydesign(id=~1, weights=~weight2, strata=NULL,data=sampledata[[1]]$phase2[[1]]) #weight alternative to prob
la_glm <- svyglm(X.X1_1~age_group+gender+living_alone, family=binomial, design=la_des)
la_fit <- summary(la_glm)

auxmat <- as.matrix(sampledata[[1]]$datares[[1]][,c("age_group","gender","living_alone")]) 
auxmat1 <- cbind(1,auxmat)
xb <- auxmat1 %*% la_fit$coefficients[,1];  
imp_X1.1 <- ifelse(exp(xb)/(1+exp(xb))>0.5,1,0)   #imputed x.x1  for all phase 1 subjects
sum(sampledata[[1]]$datares[[1]]$X.X1_1)
sampledata[[1]]$datares[[1]]$imp_X1.1 <- imp_X1.1   

xmat<-list()
for(i in 1:numbX){
  assign(paste0("colname_X", i), (1:ncoldf)[colnames(sampledata[[1]]$datares[[1]]) %in% paste0('X.X', i, "_",1:maxrep)])
  assign(paste0("X",i,"row"),as.matrix(sampledata[[1]]$datares[[1]][,get(paste0("colname_X",i)) , with = FALSE]) )
  xmat[[i]] <- as.vector(t(get(paste0("X",i,"row"))))
  
}

xmat<-do.call("cbind",xmat)
X_sample<-cbind(1,xmat[!rowSums(!is.finite(xmat)),])

sum(imp_X1.1 == sampledata[[1]]$datares[[1]]$X.X1_1)/400   #prop. of correctly est. X1.1

ind<-cumsum(c(1,sampledata[[1]]$datares[[1]]$num))[1:sample_size]
X_sample[,2][ind] <- imp_X1.1  #replacing original X1.1 with its imputations

2 .Fit the interest model Pθ,η(X) to the phase one sample using the imputed values xˆi.
#eg.  fit a logistic model (first submodel)   

ncoldf <-ncol(sampledata[[1]]$datares[[1]])
colname_cost <- (1:ncoldf)[colnames(sampledata[[1]]$datares[[1]]) %in% paste('cost2', 1:maxrep, sep='_')]
costmat<-as.matrix(sampledata[[1]]$datares[[1]][,colname_cost , with = FALSE])
costmat1<-as.vector(t(costmat))
costmat2<-costmat1[is.finite(costmat1)]
YNoZero_sample<-ifelse(costmat2>0,1,0)

idK_sample<-sort(rep(1:sample_size, sampledata[[1]]$datares[[1]]$num))

#Fitting the interest model  (l_A) using glmer & the imputed X1.1  and other variables.

la_imp<-glmer(YNoZero_sample~ X_sample[,2]+X_sample[,3]+X_sample[,4]+(1|idK_sample), family=binomial)
summary(la_imp)  # parameter estimation good


3. Construct auxiliary variables z as imputed dfbetas (7) from the model fitted in step 3; these are estimates of zopt.
#extracting dfbetas (estimated IF contribution)
inf_fun <- model.matrix(la_imp) * resid(la_imp, type="response") #these are the 'actual' auxiliary variables,   dfbetas
colnames(inf_fun) <- paste0("if",1:ncol(inf_fun))

#add an offset with random effects for coxph
#get IF first  and use these to calib. the weights. 

4. Calibrate the weights using the IFs and calibrate() in survey package.

des0 <- svydesign(id=~0, weights=~weight, data=sampledata[[1]]$datares[[1]]) #define a design  with weights
calformula <- make.formula(colnames(inf_fun))

des0if <- update(des0, if1 = inf_fun[ind,1],
if2=inf_fun[ind,2], if3=inf_fun[ind,3],
if4=inf_fun[ind,4])

cal <- calibrate(des0if, formula=~if2, pop=c(N,0)) #pop total for the auxiliary vars should be given.(for N=10000)
weights(cal)

#variance estimation code
for(i in 1:(ncol(X)+1)){
  assign(paste0("theta",i),repsum[(sample_size*i+1):(sample_size*(i+1))] )
}

logsigma_a_gr <- repsum[(sample_size+1):(sample_size*2)] #extracting grad fns
alpha0_gr<- repsum[(sample_size*2+1):(sample_size*3)]
alpha1_gr<- repsum[(sample_size*3+1):(sample_size*4)]
alpha2_gr<- repsum[(sample_size*4+1):(sample_size*5)]
alpha3_gr <- repsum[(sample_size*5+1):(sample_size*6)]

clu<-sampledata[[1]]$datares[[1]]$cluster
stra<-sampledata[[1]]$datares[[1]]$strata
wght <- sampledata[[1]]$datares[[1]]$weight
mat_gr <- data.frame(logsigma_a_gr,alpha0_gr,alpha1_gr,alpha2_gr,alpha3_gr,clu,stra,wght)
mat_gr1 <- mat_gr[1:(sample_size/2),]; mat_gr2 <- mat_gr[(sample_size/2+1):sample_size,] #already weighted  because wll is optimized. 

#mat_wgr<-mat_gr[,1:5]*mat_gr$wght #unordered wgr  
#mat_wgr$clu <- mat_gr$clu; mat_wgr$stra <- mat_gr$stra
#mat_wgr1 <- mat_wgr[1:(sample_size/2),]; mat_wgr2 <- mat_wgr[(sample_size/2+1):sample_size,]

#variance estimation
n.indiv <- 2
var_ind <- seq(1,sample_size+1,n.indiv)
num.cluster <- 2
G <- matrix(0,nrow=nparam, ncol=nparam)
for(i in 1:n.strata){ #n.strata = 100
  e_hl_1 <- colSums(mat_gr1[,1:nparam][var_ind[i]:var_ind[i+1]-1,]) #1st cluster sum in str h
  e_hl_2 <- colSums(mat_gr2[,1:nparam][var_ind[i]:var_ind[i+1]-1,]) #2nd cluster sum in str h

  e_h <- (1/num.cluster) * (e_hl_1 + e_hl_2) #colsum returns a vector
  G <- G + (num.cluster / (num.cluster -1)) * ((as.matrix(e_hl_1 - e_h) %*% t(as.matrix(e_hl_1 - e_h)))
  + (as.matrix(e_hl_2 - e_h) %*% t(as.matrix(e_hl_2 - e_h))))
  
}

invhess <- solve(opt$hessian)
sqrt(diag(invhess %*% G %*% invhess)) #no need to divide by sqrt(n)

```{r}

calibrate_fn <- function(missing_z, N, dat){
  
  #no loop as this function will run as part of AD fun which will have its own loop
  la_des <- svydesign(id=~1, weights=~weight2, strata=NULL,data=dat[[i]]$phase2[[1]])

  xnam <- paste0("X.X",missing_x, "_", nth)
  auxnam <- paste0("aux",1:numbaux)
  fmla <- as.formula(paste(get("xnam"),"~",paste(auxnam,collapse="+")))
  
  la_glm <- svyglm(fmla, family=binomial, design=la_des)
  la_fit <- summary(la_glm)
  
  ncoldf <-ncol(data)
  colname_aux <- (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste0('aux', 1:numbaux, "_", nth)] #extract auxiliary variables that will be used to predict  nth measurement of some X variable
  auxmat<-as.matrix(dat[[i]]$datares[[1]][,colname_aux , with = FALSE])

  auxmat1 <- cbind(1,auxmat)
  xb <- auxmat1  %*% la_fit$coefficients[,1]
  data$imp_pmv <- ifelse(exp(xb)/(1+exp(xb))>0.5,1,0) #imputed vals for X. what if = 0.5?
  
  xmat<-list()
  maxrep<-max(dat[[i]]$datares[[1]]$num)
  for(i in 1:numbX){ 
  assign(paste0("colname_X", i), (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste0('X.X', i, "_",1:maxrep)])
  assign(paste0("X",i,"row"),as.matrix(dat[[i]]$datares[[1]][,get(paste0("colname_X",i)) , with = FALSE]) )
  xmat[[i]] <- as.vector(t(get(paste0("X",i,"row"))))
  
}

xmat<-do.call("cbind",xmat)
X_sample<-cbind(1,xmat[!rowSums(!is.finite(xmat)),])

ind<-cumsum(c(nth,dat[[i]]$datares[[1]]$num))[1:sample_size]
X_sample[,(missing_x+1)][ind] <- imp_pmv  #replacing original pmv with its imputed vals

ncoldf <-ncol(dat[[i]]$datares[[1]])
colname_cost <- (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste('cost2', 1:maxrep, sep='_')]
costmat<-as.matrix(dat[[i]]$datares[[1]][,colname_cost , with = FALSE])
costmat1<-as.vector(t(costmat))
cost2<-costmat1[is.finite(costmat1)] #not a matrix
YNoZero_sample<-ifelse(cost2>0,1,0)

idK_sample<-sort(rep(1:sample_size, dat[[i]]$datares[[1]]$num))

xn<-paste0("X_sample[,",1:numbX,"]")
xvar_a<-c(xn,"(1|idK_sample)")
la_fml<-as.formula(paste("YNoZero_sample~",paste(xvar_a,collapse="+")))

#fit the interest model 

la_imp<-glmer(la_fml, family=binomial)

#randeff_a <- c(ranef(la_imp)$idK) #estimated a's  from l_A
randeff_a <- unlist(getME(la_imp, "b"))

#inf_fun <- model.matrix(la_imp) * resid(la_imp, type="response") 

#influence function for lA
modmat <- model.matrix(la_imp)
Ihat <- (t(modmat) %*% (modmat * la_imp$fitted.values * (1 - la_imp$fitted.values))) / nrow(modmat)
score <- modmat * resid(la_imp, type="response")
inf_fun <- score %*% solve(Ihat)
#inf_fun <- (modmat * resid(la_imp, type="response")) %*% solve(Ihat)
colnames(inf_fun) <- paste0("if",1:ncol(inf_fun))

#u' for variance estimation
score.fit <- lm(score~inf_fun)
sc <- resid(score.fit, type="response")

des0 <- svydesign(id=~0, weights=~weight, data=dat[[i]]$datares[[1]]) #define a design  with weights
calformula <- make.formula(colnames(inf_fun))

des0if <- update(des0, if1 = inf_fun[ind,1],
if2=inf_fun[ind,2], if3=inf_fun[ind,3],
if4=inf_fun[ind,4])   #make this modular
cal <- calibrate(des0if, formula=~if2, pop=c(N,0))
cal_w_a <- weights(cal)
#lB   no need to impute again as lB uses same  X predictors as lA

xvar_b<-c(xn,"randeff_a[idK_sample]", "(1|idK_sample)")
lb_fml <- as.formula(paste("cost2~",paste(xvar_b,collapse="+")))
lb_imp <- glmer(lb_fml, family=gamma)
randeff_b <- unlist(getME(lb_imp, "b"))[idK] 

#inf_fun <- model.matrix(lb_imp) * resid(lb_imp, type="response") #dfbeta for lb
modmat <- model.matrix(lb_imp)
Ihat <- solve(vcov(lb_imp)) #information matrix

inf_fun <- modmat * resid(lb_imp, type="response") %*% solve(Ihat)


colnames(inf_fun) <- paste0("if",1:ncol(inf_fun))
des0 <- svydesign(id=~0, weights=~weight, data=dat[[i]]$datares[[1]])
calformula <- make.formula(colnames(inf_fun))

#inf_fun for lB  (FIM)
#1,1 n * trigamma(shape)
#1,2(2,1) -n/scale
#2,2 shape/scale^2
  

des0if <- update(des0, if1 = inf_fun[ind,1])

cal <- calibrate(des0if, formula=  , pop=c(N,0))
cal_w_b <- weights(cal)
#lC need Z missing

lc_des <- svydesign(id=~1, weights=~weights2, strata=NULL, data=dat[[i]]$phase2[[1]])
z_nam <- paste0("Z",missing_z)
z_auxnam <- paste0("Z_aux",1:numbaux)
fmla <- as.formula(paste(get("z_nam"),"~",paste(z_auxnam,collapse="+")))
lc_glm <- svyglm(fmla, family=binomial, design=lc_des)

lc_fit <- summary(lc_glm) #use this to impute pmv
ncoldf <-ncol(dat[[i]]$datares[[1]])
colname_Z_aux <- (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste0('Z_aux', 1:numbaux)]
Z_auxmat <- as.matrix(dat[[i]]$datares[[1]][,colname_Z_aux , with = FALSE])
Z_auxmat <- cbind(1,Z_auxmat)

xb <- Z_auxmat  %*% lc_fit$coefficients[,1]
data$imp_Z <- ifelse(exp(xb)/(1+exp(xb))>0.5,1,0) 
#Using the imputd pmv, fit a coxph model

#Z matrix for the sample
ncoldfZ <-ncol(dat[[i]]$datares[[1]])
colname_costZ <- (1:ncoldfZ)[colnames(dat[[i]]$datares[[1]]) %in% paste0('Z', 1:numbX)]
zmat <- as.matrix(dat[[i]]$datares[[1]][,colname_costZ, with =FALSE])

Z_nam <- c(paste0("Z",1:numbX),"offset(randeff_a)", "offset(randeff_b)")
(fmla <- as.formula(paste("Surv(Survt,di)~",paste(Z_nam,collapse="+"))))  #use a's and b's as offsets? 
lc_imp <- coxph(fmla)
dfbeta <- residuals(lc_imp,  type='dfbeta')  #this is the IF for cox model

score <- residuals(lc_imp, type="score")
score.fit<-lm(score~dfbeta)
sc <- resid(score.fit,type="response")

cal <- calibrate()
cal_w_c <- weights(cal)
}
#this function should return influence functions,  which will be used to estimate variances or  should use influence functions to  estimate variances here.
```

```{r}
#need to run this n.iter times here or in the adfun function
#use for loop to  allow  n  missing variables eg  if missing_x > 1 or just for loop i=1
#can use this function inside adfun when type=="calibrated"

calibrate_surv <- function(missing_z, N, dat,i){
  # calibration is only for the survival model at this stage
  # weighted pred models for Z using phase2 dat
lc_des <- svydesign(id=~1, weights=~weight2, strata=NULL, data=dat[[i]]$phase2[[1]])
z_nam <- paste0("Z",missing_z) # Z1
z_auxnam <- paste0("Z_aux",1:numbaux)
fmla <- as.formula(paste(get("z_nam"),"~",paste(z_auxnam,collapse="+")))
lc_glm <- svyglm(fmla, family=binomial, design=lc_des) #predicton model using phase2 data

lc_fit <- summary(lc_glm) #use this to impute pmv
ncoldf <-ncol(dat[[i]]$datares[[1]])
colname_Z_aux <- (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste0('Z_aux', 1:numbaux)]
Z_auxmat <- as.matrix(dat[[i]]$datares[[1]][,colname_Z_aux , with = FALSE])
Z_auxmat <- cbind(1,Z_auxmat)

xb <- Z_auxmat  %*% lc_fit$coefficients[,1]
data$imp_Z <- ifelse(exp(xb)/(1+exp(xb))>0.5,1,0) 
#Using the imputd pmv, fit a coxph model

#Z matrix for the sample
ncoldfZ <-ncol(dat[[i]]$datares[[1]])
colname_Z <- (1:ncoldfZ)[colnames(dat[[i]]$datares[[1]]) %in% paste0('Z', 1:numbX)]
zmat <- as.matrix(dat[[i]]$datares[[1]][,colname_Z, with =FALSE])

#estimate the random effects using glmer
  xmat<-list()
  maxrep<-max(dat[[i]]$datares[[1]]$num)
  for(j in 1:numbX){ 
  assign(paste0("colname_X", j), (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste0('X.X', j, "_",1:maxrep)])
  assign(paste0("X",j,"row"),as.matrix(dat[[i]]$datares[[1]][,get(paste0("colname_X",j)) , with = FALSE]) )
  xmat[[j]] <- as.vector(t(get(paste0("X",j,"row"))))
  
}

xmat<-do.call("cbind",xmat)
X_sample<-cbind(1,xmat[!rowSums(!is.finite(xmat)),])

ncoldf <-ncol(dat[[i]]$datares[[1]])
colname_cost <- (1:ncoldf)[colnames(dat[[i]]$datares[[1]]) %in% paste('cost2', 1:maxrep, sep='_')]
costmat<-as.matrix(dat[[i]]$datares[[1]][,colname_cost , with = FALSE])
costmat1<-as.vector(t(costmat))
cost2<-costmat1[is.finite(costmat1)] #not a matrix
YNoZero_sample<-ifelse(cost2>0,1,0)

idK_sample<-sort(rep(1:sample_size, dat[[i]]$datares[[1]]$num))

xn<-paste0("X_sample[,",1:numbX,"]")
xvar_a<-c(xn,"(1|idK_sample)")
la_fml<-as.formula(paste("YNoZero_sample~",paste(xvar_a,collapse="+")))

#fit the interest model 

la.fit<-glmer(la_fml, family=binomial)
#glmer(YNoZero_sample~ X_sample[,2]+X_sample[,3]+X_sample[,4]+(1|idK_sample), family=binomial)
#randeff_a<-unlist(ranef(la.fit))
#randeff_a<-rnorm(400,0,1)
randeff_a <- unlist(getME(la.fit, "b")) 


xvar_b<-c(xn,"randeff_a[idK_sample]", "(1|idK_sample)")
lb_fml <- as.formula(paste("cost2~",paste(xvar_b,collapse="+")))
lb.fit <- glmer(lb_fml, family=Gamma)
randeff_b <- unlist(getME(lb.fit, "b"))

Z_nam <- c(paste0("Z",1:numbX),"offset(randeff_a)", "offset(randeff_b)")
(fmla <- as.formula(paste("Surv(Survt,di)~",paste(Z_nam,collapse="+"))))  #use a's and b's as offsets? 
lc_imp <- coxph(fmla)
inf_fun <- residuals(lc_imp,  type='dfbeta')  #this is the IF for cox model
colnames(inf_fun) <- paste0("if",1:ncol(inf_fun))
des0 <- svydesign(id=~0, weights=~weight, data=dat[[i]]$datares[[1]])
calformula <- make.formula(colnames(inf_fun))
des0if <- update(des0, if1 = inf_fun[,1],
if2=inf_fun[,2], if3=inf_fun[,3],
if4=inf_fun[,4])

cal <- calibrate(des0if, formula=~if2, pop=c(N,0))
cal_w_c <- weights(cal)

return(cal_w_c)
}
```
